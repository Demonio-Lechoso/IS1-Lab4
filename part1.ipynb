{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Database Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text function, takes an preferably unclean text as parameter\n",
    "#Tokenization is used in NLP to split paragraphs and sentences into smaller units that can be more easily assigned meaning.\n",
    "def clean(uncleanedText):\n",
    "#initializing the tokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    # initializing and importing english stopwords as well as importing stemmer class\n",
    "    stopWordsEng = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    # clean text and clean stemmed text lists\n",
    "    cleanedText = []\n",
    "    cleanedAndStemmedText = [] \n",
    "\n",
    "    # remove stock market notations \"$AAPL\", hashtags, hyperlinks,strip the \"\\n\" characters , retweet text \"RT\"\n",
    "    #Basically, removing special characters\n",
    "    uncleanedText = re.sub(r'\\$\\w*', \"\", uncleanedText)\n",
    "    uncleanedText = re.sub(r'#', '', uncleanedText)\n",
    "    uncleanedText = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', uncleanedText)\n",
    "    uncleanedText = uncleanedText.strip()\n",
    "    uncleanedText = re.sub(r'^RT[\\s]+', '', uncleanedText)\n",
    "\n",
    "    # tokenize the treated text\n",
    "    uncleanedText = tokenizer.tokenize(uncleanedText)\n",
    "\n",
    "    # Remove punctuation and stopwords by looping the words in the uncleanedText text\n",
    "    # Basically remove the stop words in the text\n",
    "    for word in uncleanedText:\n",
    "        if (word not in stopWordsEng and  word not in string.punctuation):\n",
    "            cleanedText.append(word)\n",
    "\n",
    "    # Stemming the words, then appending them to the list of clean stemmed text, by looping through every word in the clean text list\n",
    "    for word in cleanedText:\n",
    "        stemmedWord = stemmer.stem(word)  # stemming word\n",
    "        cleanedAndStemmedText.append(stemmedWord)  # append to the list\n",
    "\n",
    "    # finally return clean stemmed text\n",
    "    return cleanedAndStemmedText "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean a csv text file, save the cleaned text in a new csv file and return it \n",
    "def cleanedText():\n",
    "    if exists('cleanedNews.csv'): \n",
    "        return  pd.read_csv(r'cleanedNews.csv')\n",
    "    else: \n",
    "        #read the news csv file\n",
    "        df = pd.read_csv(r'News.csv')\n",
    "        df['News']=df['News'].apply(clean)\n",
    "        df['Fake']=df['Fake'].replace({True: 1, False: 0})\n",
    "        #save the cleaned text in the new cleaned news csv file\n",
    "        df.to_csv('cleanedNews.csv', index=False)\n",
    "        return df\n",
    "\n",
    "df = cleanedText()\n",
    "\n",
    "x = df['News'] \n",
    "y = df['Fake']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "#convert text into numerical values in order to be used in the model\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "x_train = tf_vectorizer.fit_transform(x_train).toarray()\n",
    "x_test = tf_vectorizer.transform(x_test).toarray()\n",
    "\n",
    "# Creating an SVM classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Training the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Calculating the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.706\n"
     ]
    }
   ],
   "source": [
    "#function to clean a csv text file, save the cleaned text in a new csv file and return it \n",
    "def cleanedText():\n",
    "    if exists('cleanedTweets.csv'): \n",
    "        df = pd.read_csv(r'cleanedTweets.csv')\n",
    "        return df.sample(frac = 1) \n",
    "    else: \n",
    "        #read the news csv file\n",
    "        df = pd.read_csv(r'Tweets.csv', names=['target','ids','date','flag','user','text'])\n",
    "        df[\"text\"]=df[\"text\"].apply(clean)\n",
    "        #save the cleaned text in the new cleaned news csv file\n",
    "        df.to_csv('cleanedTweets.csv', index=False)\n",
    "        return df.sample(frac = 1)\n",
    "    \n",
    "df = cleanedText()\n",
    "df = df[:5000]\n",
    "\n",
    "x = df['text'] \n",
    "y = df['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "#convert text into numerical values in order to be used in the model\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "x_train = tf_vectorizer.fit_transform(x_train).toarray()\n",
    "x_test = tf_vectorizer.transform(x_test).toarray()\n",
    "\n",
    "# Creating an SVM classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Training the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Calculating the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam/Ham Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9934747145187602\n"
     ]
    }
   ],
   "source": [
    "#function to clean a csv text file, save the cleaned text in a new csv file and return it \n",
    "def cleanedText():\n",
    "    if exists('cleanedSpam_Ham_data.csv'): \n",
    "        df = pd.read_csv(r'cleanedSpam_Ham_data.csv')\n",
    "        return df.sample(frac = 1) \n",
    "    else: \n",
    "        #read the news csv file\n",
    "        df = pd.read_csv(r'Spam_Ham_data.csv')\n",
    "        df[\"email\"]=df[\"email\"].apply(clean)\n",
    "        #save the cleaned text in the new cleaned news csv file\n",
    "        df.to_csv('cleanedSpam_Ham_data.csv', index=False)\n",
    "        return df.sample(frac = 1)\n",
    "    \n",
    "df = cleanedText()\n",
    "\n",
    "x = df['email'] \n",
    "y = df['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "#convert text into numerical values in order to be used in the model\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "x_train = tf_vectorizer.fit_transform(x_train).toarray()\n",
    "x_test = tf_vectorizer.transform(x_test).toarray()\n",
    "\n",
    "# Creating an SVM classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Training the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Calculating the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
